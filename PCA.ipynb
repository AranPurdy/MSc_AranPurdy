{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "037c5281-230c-44d9-97dd-5acbd80cd7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Configuration and Setup Complete ---\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 1: Configuration & Setup\n",
    "# =============================================================================\n",
    "# This cell contains all user settings and imports all necessary libraries.\n",
    "# Modify the file paths and parameters below to match your analysis needs.\n",
    "# =============================================================================\n",
    "\n",
    "# --- 1. General Configuration ---\n",
    "\n",
    "# 1a. File Paths\n",
    "INPUT_FILE = \"/Imputed/input/file/path\"\n",
    "OUTPUT_PATH_BASE = \"/output/file/dir\"\n",
    "PATHWAY_FILE = \"path/to/pathway/mapping.xlsx\"\n",
    "### PATHWAY_FILE = None ### If you want to skip this part of the analysis\n",
    "\n",
    "# 1b. Metabolites to Exclude\n",
    "METABOLITES_TO_EXCLUDE = ['9-Methylanthracene']\n",
    "\n",
    "# 1c. Pretreatment Method\n",
    "# Options: 'pareto', 'auto', 'log', 'log+pareto', 'log+auto'\n",
    "PRETREATMENT_METHOD = 'pareto'\n",
    "\n",
    "\n",
    "# --- 2. Sample & Timepoint Configuration ---\n",
    "\n",
    "# 2a. Sample Naming Patterns\n",
    "# Define how to identify conditions and timepoints from your sample names.\n",
    "# Use a list of (regex, condition_name) tuples.\n",
    "# The regex MUST include a named group `(?P<timepoint>\\d+)` to capture the timepoint identifier.\n",
    "SAMPLE_NAMING_PATTERNS = [\n",
    "    (r'TM2A(?P<timepoint>\\d+)_', '+ GFP'),  # Pattern for positive condition\n",
    "    (r'TM2An(?P<timepoint>\\d+)_', '- GFP')  # Pattern for negative condition\n",
    "]\n",
    "\n",
    "# 2b. Timepoint Mapping\n",
    "# Map the captured timepoint identifier (from the regex) to a display name.\n",
    "TIMEPOINT_MAP = {\n",
    "    '1': '0h',\n",
    "    '2': '0.5h',\n",
    "    '3': '2h',\n",
    "    '4': '5h',\n",
    "    '5': '10h'\n",
    "}\n",
    "\n",
    "# 2c. Timepoint Plotting Order\n",
    "# List the display names in the order you want them to appear in plots and legends.\n",
    "TIMEPOINT_PLOT_ORDER = ['0h', '0.5h', '2h', '5h', '10h']\n",
    "\n",
    "\n",
    "# --- 3. Pathway Analysis Configuration ---\n",
    "\n",
    "# 3a. Pathway File Structure\n",
    "PATHWAY_NAME_COLUMN = 0\n",
    "METABOLITES_COLUMN = 2\n",
    "\n",
    "# 3b. Metabolite Delimiter\n",
    "METABOLITE_DELIMITER = ','\n",
    "\n",
    "# 3c. Pathway Label Display Length\n",
    "PATHWAY_LABEL_MAX_LENGTH = 70\n",
    "\n",
    "\n",
    "# --- 4. Imports and Environment Setup ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "import re\n",
    "from matplotlib.patches import Ellipse\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.transforms as mtransforms\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"--- Configuration and Setup Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c2c2c53d-150b-47a7-ae38-a72a9c23213f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      ">>> INITIATING PCA ANALYSIS PIPELINE <<<\n",
      "Loading data from: /Users/aranpurdy/desktop/CFPS/PCA/RF/MOD_RF_Imputed.xlsx\n",
      "Initial data shape: (115, 50)\n",
      "Excluded 1 specified metabolite(s).\n",
      "Final data shape: (114, 50)\n",
      "\n",
      "--- Starting Data Pretreatment: pareto ---\n",
      "Applying Pareto scaling...\n",
      "\n",
      "Performing PCA...\n",
      "Explained variance ratio (Top 5): [0.3757615  0.17967533 0.09389609 0.089471   0.05766276]\n",
      "\n",
      "Performing pathway enrichment analysis...\n",
      "\n",
      "Generating plots for Overall Analysis...\n",
      "\n",
      "Analyzing timepoint: 0h\n",
      "\n",
      "--- Starting Data Pretreatment: pareto ---\n",
      "Applying Pareto scaling...\n",
      "\n",
      "Performing PCA...\n",
      "Explained variance ratio (Top 5): [0.45880013 0.19786408 0.12004526 0.06779076 0.0508915 ]\n",
      "\n",
      "Performing pathway enrichment analysis...\n",
      "Generating plots for Timepoint: 0h...\n",
      "\n",
      "Analyzing timepoint: 0.5h\n",
      "\n",
      "--- Starting Data Pretreatment: pareto ---\n",
      "Applying Pareto scaling...\n",
      "\n",
      "Performing PCA...\n",
      "Explained variance ratio (Top 5): [0.54483114 0.16928339 0.07604712 0.06533701 0.04805199]\n",
      "\n",
      "Performing pathway enrichment analysis...\n",
      "Generating plots for Timepoint: 0.5h...\n",
      "\n",
      "Analyzing timepoint: 2h\n",
      "\n",
      "--- Starting Data Pretreatment: pareto ---\n",
      "Applying Pareto scaling...\n",
      "\n",
      "Performing PCA...\n",
      "Explained variance ratio (Top 5): [0.33963434 0.26083381 0.18507089 0.06619663 0.04330409]\n",
      "\n",
      "Performing pathway enrichment analysis...\n",
      "Generating plots for Timepoint: 2h...\n",
      "\n",
      "Analyzing timepoint: 5h\n",
      "\n",
      "--- Starting Data Pretreatment: pareto ---\n",
      "Applying Pareto scaling...\n",
      "\n",
      "Performing PCA...\n",
      "Explained variance ratio (Top 5): [0.34852031 0.24772527 0.22706248 0.05874619 0.05229739]\n",
      "\n",
      "Performing pathway enrichment analysis...\n",
      "Generating plots for Timepoint: 5h...\n",
      "\n",
      "Analyzing timepoint: 10h\n",
      "\n",
      "--- Starting Data Pretreatment: pareto ---\n",
      "Applying Pareto scaling...\n",
      "\n",
      "Performing PCA...\n",
      "Explained variance ratio (Top 5): [0.33093751 0.28235089 0.13815669 0.07925705 0.05895227]\n",
      "\n",
      "Performing pathway enrichment analysis...\n",
      "Generating plots for Timepoint: 10h...\n",
      "\n",
      "PDF report saved to: /Users/aranpurdy/desktop/TEST/PCA_FINAL_PCA_Analysis_pareto.pdf\n",
      "Excel results saved to: /Users/aranpurdy/desktop/TEST/PCA_FINAL_PCA_Results_pareto.xlsx\n",
      "\n",
      "\n",
      "--- Analysis Complete! ---\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 2: Analysis Pipeline\n",
    "# =============================================================================\n",
    "# This cell contains all functions and the execution logic for the PCA.\n",
    "# Do not modify this cell. Run it after setting your parameters in Cell 1.\n",
    "# =============================================================================\n",
    "\n",
    "# --- A. Data Loading and Pretreatment Functions ---\n",
    "\n",
    "def load_data(filepath, exclude_metabolites=None):\n",
    "    \"\"\"Load Excel data with metabolites as rows and samples as columns\"\"\"\n",
    "    print(f\"Loading data from: {filepath}\")\n",
    "    try:\n",
    "        df = pd.read_excel(filepath, index_col=0)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: File not found at {filepath}. Please check the path in the configuration cell.\")\n",
    "        return None\n",
    "    print(f\"Initial data shape: {df.shape}\")\n",
    "    if exclude_metabolites:\n",
    "        initial_count = df.shape[0]\n",
    "        df = df.drop(index=exclude_metabolites, errors='ignore')\n",
    "        excluded_count = initial_count - df.shape[0]\n",
    "        print(f\"Excluded {excluded_count} specified metabolite(s).\")\n",
    "    print(f\"Final data shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "def log_transform(data):\n",
    "    \"\"\"Apply log1p transformation: log(1 + x)\"\"\"\n",
    "    print(\"Applying log transformation...\")\n",
    "    return np.log1p(data).fillna(0)\n",
    "\n",
    "def pareto_scaling(data):\n",
    "    \"\"\"Apply Pareto scaling: (x - mean) / sqrt(std)\"\"\"\n",
    "    print(\"Applying Pareto scaling...\")\n",
    "    mean_values = data.mean(axis=1)\n",
    "    std_values = data.std(axis=1, ddof=1)\n",
    "    std_values[std_values == 0] = 1\n",
    "    scaled_data = data.sub(mean_values, axis=0).div(np.sqrt(std_values), axis=0)\n",
    "    return scaled_data.replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "def auto_scaling(data):\n",
    "    \"\"\"Apply Auto scaling (Z-score): (x - mean) / std\"\"\"\n",
    "    print(\"Applying Auto scaling...\")\n",
    "    mean_values = data.mean(axis=1)\n",
    "    std_values = data.std(axis=1, ddof=1)\n",
    "    std_values[std_values == 0] = 1\n",
    "    scaled_data = data.sub(mean_values, axis=0).div(std_values, axis=0)\n",
    "    return scaled_data.replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "def apply_pretreatment(data, method):\n",
    "    \"\"\"Dispatcher function to apply the chosen pretreatment method.\"\"\"\n",
    "    print(f\"\\n--- Starting Data Pretreatment: {method} ---\")\n",
    "    if method.lower() == 'log': return log_transform(data)\n",
    "    elif method.lower() == 'pareto': return pareto_scaling(data)\n",
    "    elif method.lower() == 'auto': return auto_scaling(data)\n",
    "    elif method.lower() == 'log+pareto': return pareto_scaling(log_transform(data))\n",
    "    elif method.lower() == 'log+auto': return auto_scaling(log_transform(data))\n",
    "    else:\n",
    "        print(f\"Warning: Pretreatment method '{method}' not recognized. Returning original data.\")\n",
    "        return data\n",
    "\n",
    "# --- B. Core Analysis and Plotting Functions ---\n",
    "\n",
    "def get_groups(sample_name, patterns, timepoint_map):\n",
    "    \"\"\"Parse sample names using a list of regex patterns.\"\"\"\n",
    "    for pattern, condition in patterns:\n",
    "        match = re.search(pattern, sample_name)\n",
    "        if match:\n",
    "            try:\n",
    "                timepoint_id = match.group('timepoint')\n",
    "                timepoint_name = timepoint_map.get(timepoint_id, f\"ID:{timepoint_id}\")\n",
    "                return timepoint_name, condition\n",
    "            except IndexError:\n",
    "                print(f\"Warning: Regex pattern '{pattern}' is missing the named group '(?P<timepoint>...)'.\")\n",
    "                return \"Unknown Timepoint\", condition\n",
    "    return \"Unknown\", \"Unknown\"\n",
    "\n",
    "\n",
    "def perform_pca(scaled_data, n_components=20):\n",
    "    \"\"\"Perform PCA on scaled data.\"\"\"\n",
    "    print(\"\\nPerforming PCA...\"); data_for_pca = scaled_data.T\n",
    "    n_components = min(n_components, data_for_pca.shape[0], data_for_pca.shape[1])\n",
    "    pca = PCA(n_components=n_components); scores = pca.fit_transform(data_for_pca)\n",
    "    pc_labels = [f'PC{i+1}' for i in range(n_components)]\n",
    "    scores_df = pd.DataFrame(scores, columns=pc_labels, index=data_for_pca.index)\n",
    "    loadings = pca.components_.T\n",
    "    loadings_df = pd.DataFrame(loadings, columns=pc_labels, index=scaled_data.index)\n",
    "    print(f\"Explained variance ratio (Top 5): {pca.explained_variance_ratio_[:5]}\")\n",
    "    return pca, scores_df, loadings_df\n",
    "\n",
    "# <<< FIX: Function now accepts total_top_metabolites to calculate the new fraction >>>\n",
    "def perform_pathway_enrichment(top_metabolites, total_top_metabolites, pathway_file, name_col, met_col, delimiter):\n",
    "    \"\"\"Performs pathway analysis, calculating the fraction of top loadings per pathway.\"\"\"\n",
    "    if not pathway_file: return None\n",
    "    print(\"\\nPerforming pathway enrichment analysis...\")\n",
    "    try:\n",
    "        pathway_df = pd.read_excel(pathway_file, header=None)\n",
    "        pathway_results = []\n",
    "        top_metabolites_set = {m.lower() for m in top_metabolites}\n",
    "        for _, row in pathway_df.iterrows():\n",
    "            pathway_name = row[name_col]\n",
    "            if pd.isna(pathway_name): continue\n",
    "            \n",
    "            metabolite_str = str(row[met_col])\n",
    "            pathway_metabolites = {m.strip().lower() for m in metabolite_str.split(delimiter) if m.strip()}\n",
    "            hits = top_metabolites_set.intersection(pathway_metabolites)\n",
    "            if hits:\n",
    "                pathway_results.append({'Pathway': str(pathway_name).strip(), 'Hits': len(hits)})\n",
    "        if not pathway_results:\n",
    "            print(\"No pathway matches found.\"); return None\n",
    "        \n",
    "        results_df = pd.DataFrame(pathway_results)\n",
    "        results_df['grouping_key'] = results_df['Pathway'].str.lower()\n",
    "        agg_df = results_df.groupby('grouping_key').agg(\n",
    "            Pathway=('Pathway', 'first'), Hits=('Hits', 'sum')\n",
    "        ).reset_index(drop=True)\n",
    "        \n",
    "        # <<< FIX: Calculate the fraction of top loadings, not pathway coverage >>>\n",
    "        agg_df['Fraction'] = agg_df['Hits'] / total_top_metabolites\n",
    "        \n",
    "        return agg_df.sort_values(by=['Hits', 'Fraction'], ascending=[False, False])\n",
    "    except Exception as e:\n",
    "        print(f\"Error during pathway enrichment: {e}\"); return None\n",
    "\n",
    "# <<< FIX: Plotting function updated for the new 'Fraction' metric >>>\n",
    "def plot_pathway_enrichment(pathway_counts, ax, title, max_len, total_top_metabolites):\n",
    "    \"\"\"Plots the fraction of top loadings represented in each pathway.\"\"\"\n",
    "    if pathway_counts is None or pathway_counts.empty:\n",
    "        ax.text(0.5, 0.5, 'No Pathway Data Available', ha='center', va='center', fontsize=12); ax.axis('off'); return\n",
    "    \n",
    "    data_to_plot = pathway_counts.head(15).copy().sort_values('Fraction', ascending=True)\n",
    "    bar_colors = plt.cm.viridis_r(np.linspace(0.1, 0.9, len(data_to_plot)))\n",
    "    bars = ax.barh(y=data_to_plot['Pathway'], width=data_to_plot['Fraction'], color=bar_colors)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Fraction of Top PC Loadings in Pathway', fontsize=12)\n",
    "    ax.set_ylabel(None); ax.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "    \n",
    "    labels = [f\" {row['Hits']} / {total_top_metabolites}\" for _, row in data_to_plot.iterrows()]\n",
    "    ax.bar_label(bars, labels=labels, padding=3, fontsize=10)\n",
    "    \n",
    "    ax.set_yticklabels([(label.get_text()[:max_len-3] + '...') if len(label.get_text()) > max_len else label.get_text() for label in ax.get_yticklabels()])\n",
    "\n",
    "def add_confidence_ellipse(ax, x, y, n_std=2.0, facecolor='none', **kwargs):\n",
    "    \"\"\"Add a confidence ellipse to a scatter plot.\"\"\"\n",
    "    if len(x) < 3: return\n",
    "    cov = np.cov(x, y); pearson = cov[0, 1] / np.sqrt(cov[0, 0] * cov[1, 1])\n",
    "    ell_radius_x, ell_radius_y = np.sqrt(1 + pearson), np.sqrt(1 - pearson)\n",
    "    ellipse = Ellipse((0, 0), width=ell_radius_x * 2, height=ell_radius_y * 2, facecolor=facecolor, **kwargs)\n",
    "    scale_x, mean_x = np.sqrt(cov[0, 0]) * n_std, np.mean(x)\n",
    "    scale_y, mean_y = np.sqrt(cov[1, 1]) * n_std, np.mean(y)\n",
    "    transf = mtransforms.Affine2D().rotate_deg(45).scale(scale_x, scale_y).translate(mean_x, mean_y)\n",
    "    ellipse.set_transform(transf + ax.transData); ax.add_patch(ellipse)\n",
    "\n",
    "def timepoint_specific_pca(data, metadata, timepoint, config):\n",
    "    \"\"\"Perform a complete PCA analysis for a single timepoint.\"\"\"\n",
    "    print(f\"\\nAnalyzing timepoint: {timepoint}\")\n",
    "    mask = metadata['timepoint'] == timepoint; timepoint_data = data.loc[:, mask]\n",
    "    if timepoint_data.shape[1] < 3:\n",
    "        print(f\"Skipping {timepoint}: not enough samples.\"); return None, None, None, None, None\n",
    "    scaled_data = apply_pretreatment(timepoint_data, config['pretreatment_method'])\n",
    "    pca, scores_df, loadings_df = perform_pca(scaled_data, n_components=20)\n",
    "    top_metabolites = list(set(loadings_df['PC1'].abs().nlargest(20).index.tolist() + loadings_df['PC2'].abs().nlargest(20).index.tolist()))\n",
    "    total_top_metabolites = len(top_metabolites)\n",
    "    pathway_results = perform_pathway_enrichment(\n",
    "        top_metabolites, total_top_metabolites, config['pathway_file'], config['name_col'], config['met_col'], config['delimiter']\n",
    "    )\n",
    "    return pca, scores_df, loadings_df, pathway_results, top_metabolites\n",
    "\n",
    "def create_comprehensive_pca_report(data, config):\n",
    "    \"\"\"Create a multi-page PDF report of the PCA results.\"\"\"\n",
    "    metadata = pd.DataFrame([get_groups(s, config['patterns'], config['tp_map']) for s in data.columns], columns=['timepoint', 'condition'], index=data.columns)\n",
    "    timepoint_order = config['tp_order']\n",
    "    color_dict = dict(zip(timepoint_order, plt.cm.plasma_r(np.linspace(0.1, 0.9, len(timepoint_order)))))\n",
    "    \n",
    "    scaled_data_overall = apply_pretreatment(data, config['pretreatment_method'])\n",
    "    pca, scores_df, loadings_df = perform_pca(scaled_data_overall, n_components=20)\n",
    "    \n",
    "    top_metabolites_overall = list(set(loadings_df['PC1'].abs().nlargest(20).index.tolist() + loadings_df['PC2'].abs().nlargest(20).index.tolist()))\n",
    "    total_top_metabolites_overall = len(top_metabolites_overall)\n",
    "    \n",
    "    pathway_results_overall = perform_pathway_enrichment(\n",
    "        top_metabolites_overall, total_top_metabolites_overall, config['pathway_file'], config['name_col'], config['met_col'], config['delimiter']\n",
    "    )\n",
    "    \n",
    "    pdf_filename = f\"{config['output_path']}_PCA_Analysis_{config['pretreatment_method'].replace('+', '_')}.pdf\"\n",
    "    title_pretreatment = config['pretreatment_method'].replace(\"+\", \" + \").title()\n",
    "\n",
    "    with PdfPages(pdf_filename) as pdf:\n",
    "        print(\"\\nGenerating plots for Overall Analysis...\")\n",
    "        # Page 1 & 2 are unchanged\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10), constrained_layout=True)\n",
    "        for tp in timepoint_order:\n",
    "            for cond, marker in [('+ GFP', 'o'), ('- GFP', 's')]:\n",
    "                mask = (metadata['timepoint'] == tp) & (metadata['condition'] == cond)\n",
    "                if mask.any(): ax1.scatter(scores_df.loc[mask, 'PC1'], scores_df.loc[mask, 'PC2'], color=color_dict.get(tp, 'gray'), marker=marker, s=150, alpha=0.8, edgecolors='black', label=f'{tp} ({cond})')\n",
    "        for tp in timepoint_order:\n",
    "            if (metadata['timepoint'] == tp).sum() > 2: add_confidence_ellipse(ax1, scores_df.loc[metadata['timepoint'] == tp, 'PC1'], scores_df.loc[metadata['timepoint'] == tp, 'PC2'], edgecolor=color_dict.get(tp, 'gray'), linewidth=2)\n",
    "        ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})'); ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})'); ax1.set_title('Score Plot: PC1 vs PC2', fontsize=16, fontweight='bold'); ax1.grid(True, alpha=0.3); ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        pc_nums = np.arange(1, 11); ax2.bar(pc_nums, pca.explained_variance_ratio_[:10], color='steelblue'); ax2_twin = ax2.twinx(); ax2_twin.plot(pc_nums, np.cumsum(pca.explained_variance_ratio_[:10]), 'r-o'); ax2_twin.set_ylabel('Cumulative Variance Ratio'); ax2.set_ylabel('Explained Variance Ratio'); ax2.set_xlabel('Principal Component'); ax2.set_title('Scree Plot', fontsize=16, fontweight='bold'); ax2.set_xticks(pc_nums)\n",
    "        fig.suptitle(f'Overall PCA Analysis: {title_pretreatment}', fontsize=20, fontweight='bold'); pdf.savefig(fig, bbox_inches='tight'); plt.close(fig)\n",
    "        \n",
    "        fig, (ax_load, ax_bi) = plt.subplots(1, 2, figsize=(22, 10), constrained_layout=True)\n",
    "        top_loadings = np.sqrt(loadings_df['PC1']**2 + loadings_df['PC2']**2).nlargest(15).index; ax_load.scatter(loadings_df['PC1'], loadings_df['PC2'], alpha=0.6, c='gray')\n",
    "        for met in top_loadings: ax_load.text(loadings_df.loc[met, 'PC1'], loadings_df.loc[met, 'PC2'], met, fontsize=8, ha='center', bbox=dict(facecolor='white', alpha=0.5, boxstyle='round,pad=0.2'))\n",
    "        ax_load.set_xlabel('PC1 Loadings'); ax_load.set_ylabel('PC2 Loadings'); ax_load.set_title('Loading Plot (PC1 vs PC2)', fontweight='bold'); ax_load.axhline(0, c='grey', ls='--'); ax_load.axvline(0, c='grey', ls='--')\n",
    "        for tp in timepoint_order:\n",
    "            for cond, marker in [('+ GFP', 'o'), ('- GFP', 's')]:\n",
    "                mask = (metadata['timepoint'] == tp) & (metadata['condition'] == cond)\n",
    "                if mask.any(): ax_bi.scatter(scores_df.loc[mask, 'PC1'], scores_df.loc[mask, 'PC2'], color=color_dict.get(tp, 'gray'), marker=marker, s=50, alpha=0.5)\n",
    "        scale_factor = 0.6 * np.max(np.abs(scores_df[['PC1', 'PC2']].values)) / np.max(np.abs(loadings_df.loc[top_loadings, ['PC1', 'PC2']].values))\n",
    "        for met in top_loadings:\n",
    "            ax_bi.arrow(0, 0, loadings_df.loc[met, 'PC1']*scale_factor, loadings_df.loc[met, 'PC2']*scale_factor, color='r', head_width=0.2)\n",
    "            ax_bi.text(loadings_df.loc[met, 'PC1']*scale_factor*1.15, loadings_df.loc[met, 'PC2']*scale_factor*1.15, met, color='r', ha='center', va='center', fontsize=8)\n",
    "        ax_bi.set_xlabel(f'PC1 Scores ({pca.explained_variance_ratio_[0]:.1%})'); ax_bi.set_ylabel(f'PC2 Scores ({pca.explained_variance_ratio_[1]:.1%})'); ax_bi.set_title('Biplot', fontweight='bold'); ax_bi.axhline(0, c='grey', ls='--'); ax_bi.axvline(0, c='grey', ls='--')\n",
    "        fig.suptitle(f'Overall Loadings and Biplot: {title_pretreatment}', fontsize=20, fontweight='bold'); pdf.savefig(fig, bbox_inches='tight'); plt.close(fig)\n",
    "        \n",
    "        # --- Overall Page 3: Top Loadings & Pathway Plots ---\n",
    "        fig = plt.figure(figsize=(18, 14), constrained_layout=True); gs = fig.add_gridspec(2, 2)\n",
    "        ax1, ax2, ax3 = fig.add_subplot(gs[0, 0]), fig.add_subplot(gs[0, 1]), fig.add_subplot(gs[1, :])\n",
    "        for ax, pc, N in [(ax1, 'PC1', 0), (ax2, 'PC2', 1)]:\n",
    "            top15 = loadings_df[pc].abs().nlargest(15).index\n",
    "            sns.barplot(x=loadings_df.loc[top15, pc], y=top15, ax=ax, palette='coolwarm')\n",
    "            ax.set_title(f'Top 15 {pc} Loadings ({pca.explained_variance_ratio_[N]:.1%})', fontsize=14, fontweight='bold')\n",
    "        plot_pathway_enrichment(pathway_results_overall, ax3, 'Pathway Representation in Top PC1 & PC2 Loadings',\n",
    "                                max_len=config['max_label_len'], total_top_metabolites=total_top_metabolites_overall)\n",
    "        fig.suptitle(f'Overall Feature Importance & Pathways: {title_pretreatment}', fontsize=20, fontweight='bold'); pdf.savefig(fig, bbox_inches='tight'); plt.close(fig)\n",
    "\n",
    "        # --- Timepoint-Specific Analysis Pages ---\n",
    "        for timepoint in timepoint_order:\n",
    "            pca_tp, scores_df_tp, loadings_df_tp, pathway_results_tp, top_metabolites_tp = timepoint_specific_pca(data, metadata, timepoint, config)\n",
    "            if pca_tp is None: continue\n",
    "            total_top_metabolites_tp = len(top_metabolites_tp)\n",
    "\n",
    "            print(f\"Generating plots for Timepoint: {timepoint}...\")\n",
    "            # Page 1 & 2 for timepoints are unchanged\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10), constrained_layout=True); current_meta = metadata.loc[scores_df_tp.index]\n",
    "            for cond, marker, color in [('+ GFP', 'o', 'blue'), ('- GFP', 's', 'red')]:\n",
    "                mask = current_meta['condition'] == cond\n",
    "                if mask.any(): ax1.scatter(scores_df_tp.loc[mask, 'PC1'], scores_df_tp.loc[mask, 'PC2'], c=color, marker=marker, s=150, edgecolors='k', label=cond); add_confidence_ellipse(ax1, scores_df_tp.loc[mask, 'PC1'], scores_df_tp.loc[mask, 'PC2'], edgecolor=color, linewidth=2)\n",
    "            ax1.set_xlabel(f'PC1 ({pca_tp.explained_variance_ratio_[0]:.1%})'); ax1.set_ylabel(f'PC2 ({pca_tp.explained_variance_ratio_[1]:.1%})'); ax1.set_title('Score Plot: PC1 vs PC2', fontsize=16, fontweight='bold'); ax1.grid(True, alpha=0.3); ax1.legend()\n",
    "            pc_nums_tp = np.arange(1, 11); ax2.bar(pc_nums_tp, pca_tp.explained_variance_ratio_[:10], color='steelblue'); ax2_twin = ax2.twinx(); ax2_twin.plot(pc_nums_tp, np.cumsum(pca_tp.explained_variance_ratio_[:10]), 'r-o'); ax2_twin.set_ylabel('Cumulative Variance Ratio'); ax2.set_ylabel('Explained Variance Ratio'); ax2.set_xlabel('Principal Component'); ax2.set_title('Scree Plot', fontsize=16, fontweight='bold'); ax2.set_xticks(pc_nums_tp)\n",
    "            fig.suptitle(f'PCA for {timepoint}: {title_pretreatment}', fontsize=20, fontweight='bold'); pdf.savefig(fig, bbox_inches='tight'); plt.close(fig)\n",
    "            \n",
    "            fig, (ax_load, ax_bi) = plt.subplots(1, 2, figsize=(22, 10), constrained_layout=True); top_loadings_tp = np.sqrt(loadings_df_tp['PC1']**2 + loadings_df_tp['PC2']**2).nlargest(15).index; ax_load.scatter(loadings_df_tp['PC1'], loadings_df_tp['PC2'], alpha=0.6, c='gray')\n",
    "            for met in top_loadings_tp: ax_load.text(loadings_df_tp.loc[met, 'PC1'], loadings_df_tp.loc[met, 'PC2'], met, fontsize=8, ha='center', bbox=dict(facecolor='white', alpha=0.5, boxstyle='round,pad=0.2'))\n",
    "            ax_load.set_xlabel('PC1 Loadings'); ax_load.set_ylabel('PC2 Loadings'); ax_load.set_title('Loading Plot (PC1 vs PC2)', fontweight='bold'); ax_load.axhline(0, c='grey', ls='--'); ax_load.axvline(0, c='grey', ls='--')\n",
    "            for cond, marker, color in [('+ GFP', 'o', 'blue'), ('- GFP', 's', 'red')]:\n",
    "                mask = current_meta['condition'] == cond\n",
    "                if mask.any(): ax_bi.scatter(scores_df_tp.loc[mask, 'PC1'], scores_df_tp.loc[mask, 'PC2'], c=color, marker=marker, s=80, alpha=0.6, label=cond)\n",
    "            ax_bi.legend()\n",
    "            scale_factor_tp = 0.6 * np.max(np.abs(scores_df_tp[['PC1', 'PC2']].values)) / np.max(np.abs(loadings_df_tp.loc[top_loadings_tp, ['PC1', 'PC2']].values))\n",
    "            for met in top_loadings_tp:\n",
    "                ax_bi.arrow(0, 0, loadings_df_tp.loc[met, 'PC1']*scale_factor_tp, loadings_df_tp.loc[met, 'PC2']*scale_factor_tp, color='r', head_width=0.2)\n",
    "                ax_bi.text(loadings_df_tp.loc[met, 'PC1']*scale_factor_tp*1.15, loadings_df_tp.loc[met, 'PC2']*scale_factor_tp*1.15, met, color='r', ha='center', va='center', fontsize=8)\n",
    "            ax_bi.set_xlabel(f'PC1 Scores ({pca_tp.explained_variance_ratio_[0]:.1%})'); ax_bi.set_ylabel(f'PC2 Scores ({pca_tp.explained_variance_ratio_[1]:.1%})'); ax_bi.set_title('Biplot', fontweight='bold'); ax_bi.axhline(0, c='grey', ls='--'); ax_bi.axvline(0, c='grey', ls='--')\n",
    "            fig.suptitle(f'Loadings and Biplot for {timepoint}: {title_pretreatment}', fontsize=20, fontweight='bold'); pdf.savefig(fig, bbox_inches='tight'); plt.close(fig)\n",
    "\n",
    "            # --- Timepoint Page 3: Top Loadings & Pathway Enrichment ---\n",
    "            fig = plt.figure(figsize=(18, 14), constrained_layout=True); gs = fig.add_gridspec(2, 2)\n",
    "            ax1, ax2, ax3 = fig.add_subplot(gs[0, 0]), fig.add_subplot(gs[0, 1]), fig.add_subplot(gs[1, :])\n",
    "            for ax, pc, N in [(ax1, 'PC1', 0), (ax2, 'PC2', 1)]:\n",
    "                top15 = loadings_df_tp[pc].abs().nlargest(15).index\n",
    "                sns.barplot(x=loadings_df_tp.loc[top15, pc], y=top15, ax=ax, palette='coolwarm')\n",
    "                ax.set_title(f'Top 15 {pc} Loadings ({pca_tp.explained_variance_ratio_[N]:.1%})', fontsize=14, fontweight='bold')\n",
    "            plot_pathway_enrichment(pathway_results_tp, ax3, 'Pathway Representation in Top PC1 & PC2 Loadings',\n",
    "                                    max_len=config['max_label_len'], total_top_metabolites=total_top_metabolites_tp)\n",
    "            fig.suptitle(f'Feature Importance for {timepoint}: {title_pretreatment}', fontsize=20, fontweight='bold'); pdf.savefig(fig, bbox_inches='tight'); plt.close(fig)\n",
    "\n",
    "    print(f\"\\nPDF report saved to: {pdf_filename}\")\n",
    "    excel_filename = f\"{config['output_path']}_PCA_Results_{config['pretreatment_method'].replace('+', '_')}.xlsx\"\n",
    "    with pd.ExcelWriter(excel_filename) as writer:\n",
    "        pd.concat([metadata, scores_df], axis=1).to_excel(writer, sheet_name='Scores_Overall')\n",
    "        loadings_df.to_excel(writer, sheet_name='Loadings_Overall')\n",
    "        pd.DataFrame({'Explained_Variance_Ratio': pca.explained_variance_ratio_, 'Cumulative_Variance': np.cumsum(pca.explained_variance_ratio_)}, index=[f'PC{i+1}' for i in range(pca.n_components_)]).to_excel(writer, sheet_name='Variance_Overall')\n",
    "    print(f\"Excel results saved to: {excel_filename}\")\n",
    "\n",
    "# --- C. Main Execution Block ---\n",
    "print(\"\\n\\n>>> INITIATING PCA ANALYSIS PIPELINE <<<\")\n",
    "config = {\n",
    "    \"output_path\": OUTPUT_PATH_BASE, \"pathway_file\": PATHWAY_FILE,\n",
    "    \"pretreatment_method\": PRETREATMENT_METHOD, \"patterns\": SAMPLE_NAMING_PATTERNS,\n",
    "    \"tp_map\": TIMEPOINT_MAP, \"tp_order\": TIMEPOINT_PLOT_ORDER,\n",
    "    \"name_col\": PATHWAY_NAME_COLUMN, \"met_col\": METABOLITES_COLUMN,\n",
    "    \"delimiter\": METABOLITE_DELIMITER, \"max_label_len\": PATHWAY_LABEL_MAX_LENGTH\n",
    "}\n",
    "metabolomics_data = load_data(INPUT_FILE, METABOLITES_TO_EXCLUDE)\n",
    "if metabolomics_data is not None:\n",
    "    create_comprehensive_pca_report(data=metabolomics_data, config=config)\n",
    "    print(\"\\n\\n--- Analysis Complete! ---\")\n",
    "else:\n",
    "    print(\"\\n\\n--- Analysis Halted due to data loading error. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54675974-a826-4483-b8bc-fa02b1bc3ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
