{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5de6568-68b4-41d9-a27f-c3ccc4e881f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Configuration and Setup Complete ---\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 1: Configuration & Setup\n",
    "# =============================================================================\n",
    "# This cell contains all user settings and imports all necessary libraries.\n",
    "# Modify the file paths and parameters below to match your analysis needs.\n",
    "# =============================================================================\n",
    "\n",
    "# --- 1. General Configuration ---\n",
    "\n",
    "# 1a. File Paths\n",
    "INPUT_FILE = \"/PATH/TO\"\n",
    "OUTPUT_PATH_BASE = \"/users/aranpurdy/desktop/TEST/TESTPCA_Pathway_Specific\"\n",
    "PATHWAY_FILE = \"/users/aranpurdy/desktop/cfps/pathway_enrichment/MAN_Pathways.xlsx\"\n",
    "\n",
    "# 1b. Metabolites to Exclude\n",
    "METABOLITES_TO_EXCLUDE = ['9-Methylanthracene']\n",
    "\n",
    "# 1c. Pretreatment Method\n",
    "# Options: 'pareto', 'auto', 'log', 'log+pareto', 'log+auto'\n",
    "PRETREATMENT_METHOD = 'pareto'\n",
    "\n",
    "\n",
    "# --- 2. Analysis Scope ---\n",
    "# Specify pathways to run the PCA on. A separate analysis and output file\n",
    "# will be generated for EACH pathway in this list.\n",
    "# If this list is empty, a single PCA will be run on ALL metabolites.\n",
    "PATHWAYS_FOR_PCA = [\n",
    "    \"Amino Acid Metabolism\",\n",
    "    \"TCA cycle\"\n",
    "]\n",
    "\n",
    "\n",
    "# --- 3. Sample & Timepoint Configuration ---\n",
    "\n",
    "# 3a. Sample Naming Patterns\n",
    "SAMPLE_NAMING_PATTERNS = [\n",
    "    (r'TM2A(?P<timepoint>\\d+)_', '+ GFP'),\n",
    "    (r'TM2An(?P<timepoint>\\d+)_', '- GFP')\n",
    "]\n",
    "\n",
    "# 3b. Timepoint Mapping\n",
    "TIMEPOINT_MAP = { '1': '0h', '2': '0.5h', '3': '2h', '4': '5h', '5': '10h' }\n",
    "\n",
    "# 3c. Timepoint Plotting Order\n",
    "TIMEPOINT_PLOT_ORDER = ['0h', '0.5h', '2h', '5h', '10h']\n",
    "\n",
    "\n",
    "# --- 4. Pathway File Configuration ---\n",
    "# This is used to read the pathway file for filtering metabolites.\n",
    "PATHWAY_NAME_COLUMN = 0\n",
    "METABOLITES_COLUMN = 2\n",
    "METABOLITE_DELIMITER = ';'\n",
    "\n",
    "\n",
    "# --- 5. Imports and Environment Setup ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "import re\n",
    "from matplotlib.patches import Ellipse\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.transforms as mtransforms\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"--- Configuration and Setup Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54da54a0-b567-42b8-bebc-39fbc98afe39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      ">>> INITIATING PCA ANALYSIS PIPELINE <<<\n",
      "Loading data from: /users/aranpurdy/desktop/cfps/PCA/RF/MOD_RF_Imputed.xlsx\n",
      "Initial data shape: (115, 50)\n",
      "Excluded 1 specified metabolite(s).\n",
      "Final data shape: (114, 50)\n",
      "\n",
      "============================================================\n",
      "--- Running Analysis for Pathway: Amino Acid Metabolism ---\n",
      "============================================================\n",
      "Extracting metabolites for: Amino Acid Metabolism\n",
      "Found 37 unique metabolites in the specified pathway(s).\n",
      "\n",
      "--- Starting Data Pretreatment: pareto ---\n",
      "Applying Pareto scaling...\n",
      "\n",
      "Performing PCA...\n",
      "Explained variance ratio (Top 5): [0.3958735  0.21423675 0.13292105 0.11392365 0.04486715]\n",
      "\n",
      "Generating plots...\n",
      "\n",
      "Analyzing timepoint: 0h\n",
      "\n",
      "--- Starting Data Pretreatment: pareto ---\n",
      "Applying Pareto scaling...\n",
      "\n",
      "Performing PCA...\n",
      "Explained variance ratio (Top 5): [0.71829146 0.09399309 0.07749454 0.03475193 0.02175517]\n",
      "  - Generating plots for Timepoint: 0h...\n",
      "\n",
      "Analyzing timepoint: 0.5h\n",
      "\n",
      "--- Starting Data Pretreatment: pareto ---\n",
      "Applying Pareto scaling...\n",
      "\n",
      "Performing PCA...\n",
      "Explained variance ratio (Top 5): [0.72352815 0.09004596 0.06928622 0.04588326 0.02989195]\n",
      "  - Generating plots for Timepoint: 0.5h...\n",
      "\n",
      "Analyzing timepoint: 2h\n",
      "\n",
      "--- Starting Data Pretreatment: pareto ---\n",
      "Applying Pareto scaling...\n",
      "\n",
      "Performing PCA...\n",
      "Explained variance ratio (Top 5): [0.3358108  0.28085473 0.2116253  0.08060732 0.03812262]\n",
      "  - Generating plots for Timepoint: 2h...\n",
      "\n",
      "Analyzing timepoint: 5h\n",
      "\n",
      "--- Starting Data Pretreatment: pareto ---\n",
      "Applying Pareto scaling...\n",
      "\n",
      "Performing PCA...\n",
      "Explained variance ratio (Top 5): [0.34510441 0.3443045  0.19290069 0.05685786 0.03461673]\n",
      "  - Generating plots for Timepoint: 5h...\n",
      "\n",
      "Analyzing timepoint: 10h\n",
      "\n",
      "--- Starting Data Pretreatment: pareto ---\n",
      "Applying Pareto scaling...\n",
      "\n",
      "Performing PCA...\n",
      "Explained variance ratio (Top 5): [0.37155529 0.33980487 0.16169094 0.05558129 0.02771064]\n",
      "  - Generating plots for Timepoint: 10h...\n",
      "\n",
      "✓ PDF report saved to: /users/aranpurdy/desktop/TEST/TESTPCA_Pathway_Specific_Amino_Acid_Metabolism_PCA.pdf\n",
      "✓ Excel results saved to: /users/aranpurdy/desktop/TEST/TESTPCA_Pathway_Specific_Amino_Acid_Metabolism_PCA_Results.xlsx\n",
      "\n",
      "============================================================\n",
      "--- Running Analysis for Pathway: TCA cycle ---\n",
      "============================================================\n",
      "Extracting metabolites for: TCA cycle\n",
      "Found 5 unique metabolites in the specified pathway(s).\n",
      "\n",
      "--- Starting Data Pretreatment: pareto ---\n",
      "Applying Pareto scaling...\n",
      "\n",
      "Performing PCA...\n",
      "Explained variance ratio (Top 5): [0.88960472 0.08532149 0.02114039 0.00274163 0.00119177]\n",
      "\n",
      "Generating plots...\n",
      "\n",
      "Analyzing timepoint: 0h\n",
      "\n",
      "--- Starting Data Pretreatment: pareto ---\n",
      "Applying Pareto scaling...\n",
      "\n",
      "Performing PCA...\n",
      "Explained variance ratio (Top 5): [9.46948694e-01 4.95834350e-02 1.79677872e-03 1.31547954e-03\n",
      " 3.55612841e-04]\n",
      "  - Generating plots for Timepoint: 0h...\n",
      "\n",
      "Analyzing timepoint: 0.5h\n",
      "\n",
      "--- Starting Data Pretreatment: pareto ---\n",
      "Applying Pareto scaling...\n",
      "\n",
      "Performing PCA...\n",
      "Explained variance ratio (Top 5): [9.60973878e-01 3.57694415e-02 2.00358392e-03 1.13139961e-03\n",
      " 1.21696841e-04]\n",
      "  - Generating plots for Timepoint: 0.5h...\n",
      "\n",
      "Analyzing timepoint: 2h\n",
      "\n",
      "--- Starting Data Pretreatment: pareto ---\n",
      "Applying Pareto scaling...\n",
      "\n",
      "Performing PCA...\n",
      "Explained variance ratio (Top 5): [0.86885723 0.11626852 0.00902633 0.00494741 0.00090051]\n",
      "  - Generating plots for Timepoint: 2h...\n",
      "\n",
      "Analyzing timepoint: 5h\n",
      "\n",
      "--- Starting Data Pretreatment: pareto ---\n",
      "Applying Pareto scaling...\n",
      "\n",
      "Performing PCA...\n",
      "Explained variance ratio (Top 5): [8.88326615e-01 9.29831694e-02 1.37288280e-02 4.62492019e-03\n",
      " 3.36467714e-04]\n",
      "  - Generating plots for Timepoint: 5h...\n",
      "\n",
      "Analyzing timepoint: 10h\n",
      "\n",
      "--- Starting Data Pretreatment: pareto ---\n",
      "Applying Pareto scaling...\n",
      "\n",
      "Performing PCA...\n",
      "Explained variance ratio (Top 5): [9.46354817e-01 3.77422590e-02 1.53791186e-02 4.26529763e-04\n",
      " 9.72753965e-05]\n",
      "  - Generating plots for Timepoint: 10h...\n",
      "\n",
      "✓ PDF report saved to: /users/aranpurdy/desktop/TEST/TESTPCA_Pathway_Specific_TCA_cycle_PCA.pdf\n",
      "✓ Excel results saved to: /users/aranpurdy/desktop/TEST/TESTPCA_Pathway_Specific_TCA_cycle_PCA_Results.xlsx\n",
      "\n",
      "\n",
      "--- All Analyses Complete! ---\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 2: Analysis Pipeline\n",
    "# =============================================================================\n",
    "# This cell contains all functions and the execution logic for the PCA.\n",
    "# Do not modify this cell. Run it after setting your parameters in Cell 1.\n",
    "# =============================================================================\n",
    "\n",
    "# --- A. Data Loading and Pretreatment Functions ---\n",
    "\n",
    "def load_data(filepath, exclude_metabolites=None):\n",
    "    \"\"\"Load Excel data with metabolites as rows and samples as columns\"\"\"\n",
    "    print(f\"Loading data from: {filepath}\")\n",
    "    try:\n",
    "        df = pd.read_excel(filepath, index_col=0)\n",
    "        df.index = df.index.str.lower()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: File not found at {filepath}. Please check the path in the configuration cell.\")\n",
    "        return None\n",
    "    print(f\"Initial data shape: {df.shape}\")\n",
    "    if exclude_metabolites:\n",
    "        exclude_metabolites_lower = [m.lower() for m in exclude_metabolites]\n",
    "        initial_count = df.shape[0]\n",
    "        df = df.drop(index=exclude_metabolites_lower, errors='ignore')\n",
    "        excluded_count = initial_count - df.shape[0]\n",
    "        print(f\"Excluded {excluded_count} specified metabolite(s).\")\n",
    "    print(f\"Final data shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "def get_metabolites_from_pathways(pathway_file, pathways_to_include, name_col, met_col, delimiter):\n",
    "    \"\"\"Reads the pathway file and returns a set of all metabolites from the specified pathways.\"\"\"\n",
    "    if not pathway_file or not pathways_to_include:\n",
    "        return set()\n",
    "    print(f\"Extracting metabolites for: {', '.join(pathways_to_include)}\")\n",
    "    try:\n",
    "        df = pd.read_excel(pathway_file, header=None)\n",
    "        all_metabolites = set()\n",
    "        pathways_to_include_lower = {p.lower() for p in pathways_to_include}\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            pathway_name = str(row[name_col]).strip().lower()\n",
    "            if pathway_name in pathways_to_include_lower:\n",
    "                metabolites_str = str(row[met_col])\n",
    "                metabolites = {m.strip().lower() for m in metabolites_str.split(delimiter) if m.strip()}\n",
    "                all_metabolites.update(metabolites)\n",
    "        \n",
    "        print(f\"Found {len(all_metabolites)} unique metabolites in the specified pathway(s).\")\n",
    "        return all_metabolites\n",
    "    except Exception as e:\n",
    "        print(f\"✗ ERROR: Could not read or parse the pathway file. Details: {e}\")\n",
    "        return set()\n",
    "\n",
    "def log_transform(data):\n",
    "    \"\"\"Apply log1p transformation: log(1 + x)\"\"\"\n",
    "    print(\"Applying log transformation...\")\n",
    "    return np.log1p(data).fillna(0)\n",
    "\n",
    "def pareto_scaling(data):\n",
    "    \"\"\"Apply Pareto scaling: (x - mean) / sqrt(std)\"\"\"\n",
    "    print(\"Applying Pareto scaling...\")\n",
    "    mean_values = data.mean(axis=1)\n",
    "    std_values = data.std(axis=1, ddof=1)\n",
    "    std_values[std_values == 0] = 1\n",
    "    scaled_data = data.sub(mean_values, axis=0).div(np.sqrt(std_values), axis=0)\n",
    "    return scaled_data.replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "def auto_scaling(data):\n",
    "    \"\"\"Apply Auto scaling (Z-score): (x - mean) / std\"\"\"\n",
    "    print(\"Applying Auto scaling...\")\n",
    "    mean_values = data.mean(axis=1)\n",
    "    std_values = data.std(axis=1, ddof=1)\n",
    "    std_values[std_values == 0] = 1\n",
    "    scaled_data = data.sub(mean_values, axis=0).div(std_values, axis=0)\n",
    "    return scaled_data.replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "def apply_pretreatment(data, method):\n",
    "    \"\"\"Dispatcher function to apply the chosen pretreatment method.\"\"\"\n",
    "    print(f\"\\n--- Starting Data Pretreatment: {method} ---\")\n",
    "    if data.empty:\n",
    "        print(\"Warning: Data is empty, skipping pretreatment.\")\n",
    "        return data\n",
    "    if method.lower() == 'log': return log_transform(data)\n",
    "    elif method.lower() == 'pareto': return pareto_scaling(data)\n",
    "    elif method.lower() == 'auto': return auto_scaling(data)\n",
    "    elif method.lower() == 'log+pareto': return pareto_scaling(log_transform(data))\n",
    "    elif method.lower() == 'log+auto': return auto_scaling(log_transform(data))\n",
    "    else: return data\n",
    "\n",
    "# --- B. Core Analysis and Plotting Functions ---\n",
    "\n",
    "def get_groups(sample_name, patterns, timepoint_map):\n",
    "    \"\"\"Parse sample names using a list of regex patterns.\"\"\"\n",
    "    for pattern, condition in patterns:\n",
    "        match = re.search(pattern, sample_name)\n",
    "        if match:\n",
    "            try:\n",
    "                timepoint_id = match.group('timepoint')\n",
    "                timepoint_name = timepoint_map.get(timepoint_id, f\"ID:{timepoint_id}\")\n",
    "                return timepoint_name, condition\n",
    "            except IndexError: return \"Unknown Timepoint\", condition\n",
    "    return \"Unknown\", \"Unknown\"\n",
    "\n",
    "\n",
    "def perform_pca(scaled_data, n_components=20):\n",
    "    \"\"\"Perform PCA on scaled data.\"\"\"\n",
    "    if scaled_data.empty or scaled_data.shape[0] < 2:\n",
    "        print(\"Not enough data to perform PCA. A minimum of 2 metabolites is required.\"); return None, None, None\n",
    "    if scaled_data.shape[1] < 2:\n",
    "        print(\"Not enough data to perform PCA. A minimum of 2 samples is required.\"); return None, None, None\n",
    "    print(\"\\nPerforming PCA...\"); data_for_pca = scaled_data.T\n",
    "    n_components = min(n_components, data_for_pca.shape[0], data_for_pca.shape[1])\n",
    "    pca = PCA(n_components=n_components); scores = pca.fit_transform(data_for_pca)\n",
    "    pc_labels = [f'PC{i+1}' for i in range(n_components)]\n",
    "    scores_df = pd.DataFrame(scores, columns=pc_labels, index=data_for_pca.index)\n",
    "    loadings = pca.components_.T\n",
    "    loadings_df = pd.DataFrame(loadings, columns=pc_labels, index=scaled_data.index)\n",
    "    print(f\"Explained variance ratio (Top 5): {pca.explained_variance_ratio_[:5]}\")\n",
    "    return pca, scores_df, loadings_df\n",
    "\n",
    "def add_confidence_ellipse(ax, x, y, n_std=2.0, facecolor='none', **kwargs):\n",
    "    \"\"\"Add a confidence ellipse to a scatter plot.\"\"\"\n",
    "    if len(x) < 3: return\n",
    "    cov = np.cov(x, y); pearson = cov[0, 1] / np.sqrt(cov[0, 0] * cov[1, 1])\n",
    "    ell_radius_x, ell_radius_y = np.sqrt(1 + pearson), np.sqrt(1 - pearson)\n",
    "    ellipse = Ellipse((0, 0), width=ell_radius_x * 2, height=ell_radius_y * 2, facecolor=facecolor, **kwargs)\n",
    "    scale_x, mean_x = np.sqrt(cov[0, 0]) * n_std, np.mean(x)\n",
    "    scale_y, mean_y = np.sqrt(cov[1, 1]) * n_std, np.mean(y)\n",
    "    transf = mtransforms.Affine2D().rotate_deg(45).scale(scale_x, scale_y).translate(mean_x, mean_y)\n",
    "    ellipse.set_transform(transf + ax.transData); ax.add_patch(ellipse)\n",
    "\n",
    "def timepoint_specific_pca(data, metadata, timepoint, config):\n",
    "    \"\"\"Perform a complete PCA analysis for a single timepoint.\"\"\"\n",
    "    print(f\"\\nAnalyzing timepoint: {timepoint}\")\n",
    "    mask = metadata['timepoint'] == timepoint; timepoint_data = data.loc[:, mask]\n",
    "    if timepoint_data.shape[1] < 3:\n",
    "        print(f\"Skipping {timepoint}: not enough samples.\"); return None, None, None\n",
    "    scaled_data = apply_pretreatment(timepoint_data, config['pretreatment_method'])\n",
    "    pca, scores_df, loadings_df = perform_pca(scaled_data, n_components=20)\n",
    "    return pca, scores_df, loadings_df\n",
    "\n",
    "# <<< FIX: Renamed function to reflect its role for a single analysis run >>>\n",
    "def run_single_pca_analysis(data_for_analysis, config, analysis_scope_title):\n",
    "    \"\"\"Create a multi-page PDF report for a given dataset (all metabolites or a single pathway).\"\"\"\n",
    "    metadata = pd.DataFrame([get_groups(s, config['patterns'], config['tp_map']) for s in data_for_analysis.columns],\n",
    "                            columns=['timepoint', 'condition'], index=data_for_analysis.columns)\n",
    "    timepoint_order = config['tp_order']\n",
    "    color_dict = dict(zip(timepoint_order, plt.cm.plasma_r(np.linspace(0.1, 0.9, len(timepoint_order)))))\n",
    "    \n",
    "    scaled_data_overall = apply_pretreatment(data_for_analysis, config['pretreatment_method'])\n",
    "    pca, scores_df, loadings_df = perform_pca(scaled_data_overall)\n",
    "\n",
    "    if pca is None:\n",
    "        print(\"✗ Halting report generation as PCA could not be performed on the overall dataset.\")\n",
    "        return\n",
    "\n",
    "    pdf_filename = f\"{config['output_path']}_PCA.pdf\"\n",
    "    title_pretreatment = config['pretreatment_method'].replace(\"+\", \" + \").title()\n",
    "\n",
    "    with PdfPages(pdf_filename) as pdf:\n",
    "        print(\"\\nGenerating plots...\")\n",
    "        # --- Overall Page 1: Score Plot & Scree Plot ---\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10), constrained_layout=True)\n",
    "        fig.suptitle(f'Overall PCA Analysis: {title_pretreatment}\\n({analysis_scope_title})', fontsize=20, fontweight='bold')\n",
    "        for tp in timepoint_order:\n",
    "            for cond, marker in [('+ GFP', 'o'), ('- GFP', 's')]:\n",
    "                mask = (metadata['timepoint'] == tp) & (metadata['condition'] == cond)\n",
    "                if mask.any(): ax1.scatter(scores_df.loc[mask, 'PC1'], scores_df.loc[mask, 'PC2'], color=color_dict.get(tp, 'gray'), marker=marker, s=150, alpha=0.8, edgecolors='black', label=f'{tp} ({cond})')\n",
    "        for tp in timepoint_order:\n",
    "            if (metadata['timepoint'] == tp).sum() > 2: add_confidence_ellipse(ax1, scores_df.loc[metadata['timepoint'] == tp, 'PC1'], scores_df.loc[metadata['timepoint'] == tp, 'PC2'], edgecolor=color_dict.get(tp, 'gray'), linewidth=2)\n",
    "        ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})'); ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})'); ax1.set_title('Score Plot: PC1 vs PC2', fontsize=16, fontweight='bold'); ax1.grid(True, alpha=0.3); ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        \n",
    "        pc_nums = np.arange(1, min(11, len(pca.explained_variance_ratio_) + 1)); ax2.bar(pc_nums, pca.explained_variance_ratio_[:10], color='steelblue'); ax2_twin = ax2.twinx(); ax2_twin.plot(pc_nums, np.cumsum(pca.explained_variance_ratio_[:10]), 'r-o'); ax2_twin.set_ylabel('Cumulative Variance Ratio'); ax2.set_ylabel('Explained Variance Ratio'); ax2.set_xlabel('Principal Component'); ax2.set_title('Scree Plot', fontsize=16, fontweight='bold'); ax2.set_xticks(pc_nums)\n",
    "        pdf.savefig(fig, bbox_inches='tight'); plt.close(fig)\n",
    "\n",
    "        # --- Overall Page 2: Loading Plot & Biplot ---\n",
    "        fig, (ax_load, ax_bi) = plt.subplots(1, 2, figsize=(22, 10), constrained_layout=True)\n",
    "        fig.suptitle(f'Overall Loadings and Biplot: {title_pretreatment}\\n({analysis_scope_title})', fontsize=20, fontweight='bold')\n",
    "        top_loadings = np.sqrt(loadings_df['PC1']**2 + loadings_df['PC2']**2).nlargest(15).index; ax_load.scatter(loadings_df['PC1'], loadings_df['PC2'], alpha=0.6, c='gray')\n",
    "        for met in top_loadings: ax_load.text(loadings_df.loc[met, 'PC1'], loadings_df.loc[met, 'PC2'], met.title(), fontsize=8, ha='center', bbox=dict(facecolor='white', alpha=0.5, boxstyle='round,pad=0.2'))\n",
    "        ax_load.set_xlabel('PC1 Loadings'); ax_load.set_ylabel('PC2 Loadings'); ax_load.set_title('Loading Plot (PC1 vs PC2)', fontweight='bold'); ax_load.axhline(0, c='grey', ls='--'); ax_load.axvline(0, c='grey', ls='--')\n",
    "        \n",
    "        for tp in timepoint_order:\n",
    "            for cond, marker in [('+ GFP', 'o'), ('- GFP', 's')]:\n",
    "                mask = (metadata['timepoint'] == tp) & (metadata['condition'] == cond)\n",
    "                if mask.any(): ax_bi.scatter(scores_df.loc[mask, 'PC1'], scores_df.loc[mask, 'PC2'], color=color_dict.get(tp, 'gray'), marker=marker, s=50, alpha=0.5)\n",
    "        scale_factor = 0.6 * np.max(np.abs(scores_df[['PC1', 'PC2']].values)) / np.max(np.abs(loadings_df.loc[top_loadings, ['PC1', 'PC2']].values))\n",
    "        for met in top_loadings:\n",
    "            ax_bi.arrow(0, 0, loadings_df.loc[met, 'PC1']*scale_factor, loadings_df.loc[met, 'PC2']*scale_factor, color='r', head_width=0.2)\n",
    "            ax_bi.text(loadings_df.loc[met, 'PC1']*scale_factor*1.15, loadings_df.loc[met, 'PC2']*scale_factor*1.15, met.title(), color='r', ha='center', va='center', fontsize=8)\n",
    "        ax_bi.set_xlabel(f'PC1 Scores ({pca.explained_variance_ratio_[0]:.1%})'); ax_bi.set_ylabel(f'PC2 Scores ({pca.explained_variance_ratio_[1]:.1%})'); ax_bi.set_title('Biplot', fontweight='bold'); ax_bi.axhline(0, c='grey', ls='--'); ax_bi.axvline(0, c='grey', ls='--')\n",
    "        pdf.savefig(fig, bbox_inches='tight'); plt.close(fig)\n",
    "\n",
    "        # --- Timepoint-Specific Analysis Pages ---\n",
    "        for timepoint in timepoint_order:\n",
    "            pca_tp, scores_df_tp, loadings_df_tp = timepoint_specific_pca(data_for_analysis, metadata, timepoint, config)\n",
    "            if pca_tp is None: continue\n",
    "            \n",
    "            print(f\"  - Generating plots for Timepoint: {timepoint}...\")\n",
    "            # --- Timepoint Page 1: Score & Scree Plot ---\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10), constrained_layout=True); current_meta = metadata.loc[scores_df_tp.index]\n",
    "            fig.suptitle(f'PCA for {timepoint}: {title_pretreatment}\\n({analysis_scope_title})', fontsize=20, fontweight='bold')\n",
    "            for cond, marker, color in [('+ GFP', 'o', 'blue'), ('- GFP', 's', 'red')]:\n",
    "                mask = current_meta['condition'] == cond\n",
    "                if mask.any(): ax1.scatter(scores_df_tp.loc[mask, 'PC1'], scores_df_tp.loc[mask, 'PC2'], c=color, marker=marker, s=150, edgecolors='k', label=cond); add_confidence_ellipse(ax1, scores_df_tp.loc[mask, 'PC1'], scores_df_tp.loc[mask, 'PC2'], edgecolor=color, linewidth=2)\n",
    "            ax1.set_xlabel(f'PC1 ({pca_tp.explained_variance_ratio_[0]:.1%})'); ax1.set_ylabel(f'PC2 ({pca_tp.explained_variance_ratio_[1]:.1%})'); ax1.set_title('Score Plot: PC1 vs PC2', fontsize=16, fontweight='bold'); ax1.grid(True, alpha=0.3); ax1.legend()\n",
    "            \n",
    "            pc_nums_tp = np.arange(1, min(11, len(pca_tp.explained_variance_ratio_) + 1)); ax2.bar(pc_nums_tp, pca_tp.explained_variance_ratio_[:10], color='steelblue'); ax2_twin = ax2.twinx(); ax2_twin.plot(pc_nums_tp, np.cumsum(pca_tp.explained_variance_ratio_[:10]), 'r-o'); ax2_twin.set_ylabel('Cumulative Variance Ratio'); ax2.set_ylabel('Explained Variance Ratio'); ax2.set_xlabel('Principal Component'); ax2.set_title('Scree Plot', fontsize=16, fontweight='bold'); ax2.set_xticks(pc_nums_tp)\n",
    "            pdf.savefig(fig, bbox_inches='tight'); plt.close(fig)\n",
    "            \n",
    "            # --- Timepoint Page 2: Loading Plot & Biplot ---\n",
    "            fig, (ax_load, ax_bi) = plt.subplots(1, 2, figsize=(22, 10), constrained_layout=True); top_loadings_tp = np.sqrt(loadings_df_tp['PC1']**2 + loadings_df_tp['PC2']**2).nlargest(15).index\n",
    "            fig.suptitle(f'Loadings and Biplot for {timepoint}: {title_pretreatment}\\n({analysis_scope_title})', fontsize=20, fontweight='bold')\n",
    "            ax_load.scatter(loadings_df_tp['PC1'], loadings_df_tp['PC2'], alpha=0.6, c='gray')\n",
    "            for met in top_loadings_tp: ax_load.text(loadings_df_tp.loc[met, 'PC1'], loadings_df_tp.loc[met, 'PC2'], met.title(), fontsize=8, ha='center', bbox=dict(facecolor='white', alpha=0.5, boxstyle='round,pad=0.2'))\n",
    "            ax_load.set_xlabel('PC1 Loadings'); ax_load.set_ylabel('PC2 Loadings'); ax_load.set_title('Loading Plot (PC1 vs PC2)', fontweight='bold'); ax_load.axhline(0, c='grey', ls='--'); ax_load.axvline(0, c='grey', ls='--')\n",
    "            \n",
    "            for cond, marker, color in [('+ GFP', 'o', 'blue'), ('- GFP', 's', 'red')]:\n",
    "                mask = current_meta['condition'] == cond\n",
    "                if mask.any(): ax_bi.scatter(scores_df_tp.loc[mask, 'PC1'], scores_df_tp.loc[mask, 'PC2'], c=color, marker=marker, s=80, alpha=0.6, label=cond)\n",
    "            ax_bi.legend()\n",
    "            scale_factor_tp = 0.6 * np.max(np.abs(scores_df_tp[['PC1', 'PC2']].values)) / np.max(np.abs(loadings_df_tp.loc[top_loadings_tp, ['PC1', 'PC2']].values))\n",
    "            for met in top_loadings_tp:\n",
    "                ax_bi.arrow(0, 0, loadings_df_tp.loc[met, 'PC1']*scale_factor_tp, loadings_df_tp.loc[met, 'PC2']*scale_factor_tp, color='r', head_width=0.2)\n",
    "                ax_bi.text(loadings_df_tp.loc[met, 'PC1']*scale_factor_tp*1.15, loadings_df_tp.loc[met, 'PC2']*scale_factor_tp*1.15, met.title(), color='r', ha='center', va='center', fontsize=8)\n",
    "            ax_bi.set_xlabel(f'PC1 Scores ({pca_tp.explained_variance_ratio_[0]:.1%})'); ax_bi.set_ylabel(f'PC2 Scores ({pca_tp.explained_variance_ratio_[1]:.1%})'); ax_bi.set_title('Biplot', fontweight='bold'); ax_bi.axhline(0, c='grey', ls='--'); ax_bi.axvline(0, c='grey', ls='--')\n",
    "            pdf.savefig(fig, bbox_inches='tight'); plt.close(fig)\n",
    "\n",
    "    print(f\"\\n✓ PDF report saved to: {pdf_filename}\")\n",
    "    excel_filename = f\"{config['output_path']}_PCA_Results.xlsx\"\n",
    "    with pd.ExcelWriter(excel_filename) as writer:\n",
    "        pd.concat([metadata, scores_df], axis=1).to_excel(writer, sheet_name='Scores_Overall')\n",
    "        loadings_df.to_excel(writer, sheet_name='Loadings_Overall')\n",
    "        pd.DataFrame({'Explained_Variance_Ratio': pca.explained_variance_ratio_, 'Cumulative_Variance': np.cumsum(pca.explained_variance_ratio_)}, index=[f'PC{i+1}' for i in range(pca.n_components_)]).to_excel(writer, sheet_name='Variance_Overall')\n",
    "    print(f\"✓ Excel results saved to: {excel_filename}\")\n",
    "\n",
    "# --- C. Main Execution Block ---\n",
    "print(\"\\n\\n>>> INITIATING PCA ANALYSIS PIPELINE <<<\")\n",
    "config = {\n",
    "    \"output_path\": OUTPUT_PATH_BASE, \"pathway_file\": PATHWAY_FILE,\n",
    "    \"pretreatment_method\": PRETREATMENT_METHOD, \"patterns\": SAMPLE_NAMING_PATTERNS,\n",
    "    \"tp_map\": TIMEPOINT_MAP, \"tp_order\": TIMEPOINT_PLOT_ORDER,\n",
    "    \"name_col\": PATHWAY_NAME_COLUMN, \"met_col\": METABOLITES_COLUMN,\n",
    "    \"delimiter\": METABOLITE_DELIMITER,\n",
    "    \"pathways_for_pca\": PATHWAYS_FOR_PCA\n",
    "}\n",
    "metabolomics_data = load_data(INPUT_FILE, METABOLITES_TO_EXCLUDE)\n",
    "if metabolomics_data is not None:\n",
    "    # <<< FIX: New execution loop for pathway-by-pathway analysis >>>\n",
    "    pathways_to_run = config['pathways_for_pca']\n",
    "    if not pathways_to_run:\n",
    "        print(\"\\n--- Running Single Analysis on All Metabolites ---\")\n",
    "        run_single_pca_analysis(metabolomics_data, config.copy(), \"All Metabolites\")\n",
    "    else:\n",
    "        for pathway_name in pathways_to_run:\n",
    "            print(f\"\\n{'='*60}\\n--- Running Analysis for Pathway: {pathway_name} ---\\n{'='*60}\")\n",
    "            metabolites_to_keep = get_metabolites_from_pathways(\n",
    "                config['pathway_file'], [pathway_name], config['name_col'],\n",
    "                config['met_col'], config['delimiter']\n",
    "            )\n",
    "            data_for_this_pathway = metabolomics_data[metabolomics_data.index.isin(metabolites_to_keep)]\n",
    "            \n",
    "            if data_for_this_pathway.shape[0] < 2:\n",
    "                print(f\"✗ Skipping pathway '{pathway_name}': Not enough matching metabolites found in data ({data_for_this_pathway.shape[0]}).\")\n",
    "                continue\n",
    "            \n",
    "            pathway_config = config.copy()\n",
    "            safe_pathway_name = re.sub(r'[\\s/\\\\*?:\"<>|]+', '_', pathway_name)[:50]\n",
    "            pathway_config['output_path'] = f\"{OUTPUT_PATH_BASE}_{safe_pathway_name}\"\n",
    "            \n",
    "            run_single_pca_analysis(data_for_this_pathway, pathway_config, f\"Pathway: {pathway_name}\")\n",
    "\n",
    "    print(\"\\n\\n--- All Analyses Complete! ---\")\n",
    "else:\n",
    "    print(\"\\n\\n--- Analysis Halted due to data loading error. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f25047c-639a-41fa-8850-712a9ba563fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
